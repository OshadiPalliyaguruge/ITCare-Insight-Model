{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore individual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19963, 17)\n",
      "No of Rows: 19963\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   5\n",
      "Resolution                            174\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             202\n",
      "Service*+                            4366\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                 2857\n",
      "Organization                           11\n",
      "Department                             20\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2018_1.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2575, 17)\n",
      "No of Rows: 2575\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1      1\n",
      "Incident ID*+                          2\n",
      "Submit Date                            2\n",
      "Summary*                               2\n",
      "Notes                                  2\n",
      "Resolution                            13\n",
      "Assigned Group*+                       2\n",
      "Assignee+                             11\n",
      "Service*+                            203\n",
      "Submitter*                             2\n",
      "Status*                                2\n",
      "First Name+                            2\n",
      "Last Name+                             2\n",
      "Priority*                              2\n",
      "SLM Real Time Status                 433\n",
      "Organization                           3\n",
      "Department                             6\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2018_2.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22538, 17)\n",
      "No of Rows: 22538\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       3\n",
      "Incident ID*+                           4\n",
      "Submit Date                             4\n",
      "Summary*                                4\n",
      "Notes                                   7\n",
      "Resolution                            187\n",
      "Assigned Group*+                        4\n",
      "Assignee+                             213\n",
      "Service*+                            4569\n",
      "Submitter*                              4\n",
      "Status*                                 4\n",
      "First Name+                             4\n",
      "Last Name+                              4\n",
      "Priority*                               4\n",
      "SLM Real Time Status                 3290\n",
      "Organization                           14\n",
      "Department                             26\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2018_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "      Operational Categorization Tier 1 Incident ID*+ Submit Date Summary*  \\\n",
      "22536                               NaN           NaN         NaN      NaN   \n",
      "22537                               NaN           NaN         NaN      NaN   \n",
      "\n",
      "      Notes Resolution Assigned Group*+ Assignee+ Service*+ Submitter*  \\\n",
      "22536   NaN        NaN              NaN       NaN       NaN        NaN   \n",
      "22537   NaN        NaN              NaN       NaN       NaN        NaN   \n",
      "\n",
      "      Status* First Name+ Last Name+ Priority* SLM Real Time Status  \\\n",
      "22536     NaN         NaN        NaN       NaN                  NaN   \n",
      "22537     NaN         NaN        NaN       NaN                  NaN   \n",
      "\n",
      "      Organization Department  \n",
      "22536          NaN        NaN  \n",
      "22537          NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "# Find and display duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operational Categorization Tier 1</th>\n",
       "      <th>Incident ID*+</th>\n",
       "      <th>Submit Date</th>\n",
       "      <th>Summary*</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Assigned Group*+</th>\n",
       "      <th>Assignee+</th>\n",
       "      <th>Service*+</th>\n",
       "      <th>Submitter*</th>\n",
       "      <th>Status*</th>\n",
       "      <th>First Name+</th>\n",
       "      <th>Last Name+</th>\n",
       "      <th>Priority*</th>\n",
       "      <th>SLM Real Time Status</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000416368</td>\n",
       "      <td>31/12/2018 06:32:14 AM</td>\n",
       "      <td>ASYCUDA Issue- 6. One User</td>\n",
       "      <td>1. Error Message \\n ASYKUDA is not functioning...</td>\n",
       "      <td>cleared the thinapp. and browder history.</td>\n",
       "      <td>SD Incident Handling</td>\n",
       "      <td>Sathischandra B.D.</td>\n",
       "      <td>ASYCUDA</td>\n",
       "      <td>11440</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Donalad</td>\n",
       "      <td>Jayaratne</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CARGO</td>\n",
       "      <td>CARGO SALES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000416367</td>\n",
       "      <td>31/12/2018 06:29:19 AM</td>\n",
       "      <td>Email Service Not Available</td>\n",
       "      <td>Error  - outlook is not opening \\n2018\\nULKBL0...</td>\n",
       "      <td>Âoutlook/e-mail  profile createdÂ.</td>\n",
       "      <td>SD Incident Handling</td>\n",
       "      <td>Sathischandra B.D.</td>\n",
       "      <td>Email</td>\n",
       "      <td>11440</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>Dissanayake</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>AIRCRAFT MAINTENANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000416366</td>\n",
       "      <td>31/12/2018 06:27:18 AM</td>\n",
       "      <td>ORACLE ISSUE - 7. One User</td>\n",
       "      <td>Error Message:\\n Oracle is out of order \\n2132...</td>\n",
       "      <td>refreshed session and cleared the browser</td>\n",
       "      <td>SD Incident Handling</td>\n",
       "      <td>Sathischandra B.D.</td>\n",
       "      <td>Oracle EBS</td>\n",
       "      <td>11440</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Praneeth</td>\n",
       "      <td>De Alwis</td>\n",
       "      <td>Low</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>ENGINEERING MATERIALS &amp; SUPPLY CHAIN MANAGEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000416364</td>\n",
       "      <td>31/12/2018 06:21:53 AM</td>\n",
       "      <td>PC related issues Â Airport</td>\n",
       "      <td>dialog tv is out of order. \\ns/diva\\n4973\\nseedya</td>\n",
       "      <td>payment not done</td>\n",
       "      <td>DCS</td>\n",
       "      <td>Sudath Rohana ROHANA</td>\n",
       "      <td>Desktop Service</td>\n",
       "      <td>11440</td>\n",
       "      <td>Closed</td>\n",
       "      <td>AIRPORT</td>\n",
       "      <td>USER</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Within the Service Target</td>\n",
       "      <td>AIRPORT &amp; GROUND SERVICES</td>\n",
       "      <td>AIRPORT SERVICE DELIVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000416363</td>\n",
       "      <td>31/12/2018 06:19:02 AM</td>\n",
       "      <td>BTP Not working</td>\n",
       "      <td>Counter and Airline - c 07. \\n4507\\nUL\\nAsanka</td>\n",
       "      <td>printer emulator restarted</td>\n",
       "      <td>DCS</td>\n",
       "      <td>Sudath Rohana ROHANA</td>\n",
       "      <td>File and Print Service</td>\n",
       "      <td>11440</td>\n",
       "      <td>Closed</td>\n",
       "      <td>AIRPORT</td>\n",
       "      <td>USER</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Within the Service Target</td>\n",
       "      <td>AIRPORT &amp; GROUND SERVICES</td>\n",
       "      <td>AIRPORT SERVICE DELIVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22532</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000351145</td>\n",
       "      <td>1/1/2018 8:48</td>\n",
       "      <td>Printer Issue Airport Â Laser Network</td>\n",
       "      <td>Counter 15 printer issue\\nContact Number  4508...</td>\n",
       "      <td>They had replaced torner after explaining that...</td>\n",
       "      <td>DCS</td>\n",
       "      <td>Upul Gunaratne GUNARATNE</td>\n",
       "      <td>File and Print Service</td>\n",
       "      <td>C13038</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Sachinika Fernando</td>\n",
       "      <td>FERNANDO</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Within the Service Target</td>\n",
       "      <td>AIRPORT &amp; GROUND SERVICES</td>\n",
       "      <td>AIRPORT SERVICE DELIVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22533</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000351144</td>\n",
       "      <td>1/1/2018 8:39</td>\n",
       "      <td>Printer Issue tÂ Laser Network</td>\n",
       "      <td>Multiple Printers are not working maintenance ...</td>\n",
       "      <td>replaced the fuser unit.</td>\n",
       "      <td>DMS Printer Support</td>\n",
       "      <td>DMS Printer Support</td>\n",
       "      <td>File and Print Service</td>\n",
       "      <td>C13038</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Mahinda Karunachandra</td>\n",
       "      <td>KARUNACHANDRA</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Service Targets Breached</td>\n",
       "      <td>ENGINEERING</td>\n",
       "      <td>AIRCRAFT MAINTENANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22534</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000351142</td>\n",
       "      <td>1/1/2018 8:23</td>\n",
       "      <td>Duty Free System Issue -  Mobile Devices</td>\n",
       "      <td>Device Type - mobile device.\\n\\nDevice Number(...</td>\n",
       "      <td>Advise user to correctly assign his position t...</td>\n",
       "      <td>Duty Free System Support</td>\n",
       "      <td>Ishan Kularathna KULARATHNA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C12943</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Primrose Arendtsz</td>\n",
       "      <td>ARENDTSZ</td>\n",
       "      <td>Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CABIN SERVICES</td>\n",
       "      <td>INFLIGHT SERVICE DELIVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22535</th>\n",
       "      <td>Failure</td>\n",
       "      <td>INC000000351140</td>\n",
       "      <td>1/1/2018 8:19</td>\n",
       "      <td>BTP Not working</td>\n",
       "      <td>Counter 28</td>\n",
       "      <td>BTP working fine. KB repalced</td>\n",
       "      <td>DCS</td>\n",
       "      <td>Dilanka Chandrasekara CHANDRASEKARA</td>\n",
       "      <td>File and Print Service</td>\n",
       "      <td>21977</td>\n",
       "      <td>Closed</td>\n",
       "      <td>AIRPORT</td>\n",
       "      <td>USER</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Within the Service Target</td>\n",
       "      <td>AIRPORT &amp; GROUND SERVICES</td>\n",
       "      <td>AIRPORT SERVICE DELIVERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22536 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Operational Categorization Tier 1    Incident ID*+  \\\n",
       "0                               Failure  INC000000416368   \n",
       "1                               Failure  INC000000416367   \n",
       "2                               Failure  INC000000416366   \n",
       "3                               Failure  INC000000416364   \n",
       "4                               Failure  INC000000416363   \n",
       "...                                 ...              ...   \n",
       "22532                           Failure  INC000000351145   \n",
       "22533                           Failure  INC000000351144   \n",
       "22534                           Failure  INC000000351142   \n",
       "22535                           Failure  INC000000351140   \n",
       "22537                               NaN              NaN   \n",
       "\n",
       "                  Submit Date                                  Summary*  \\\n",
       "0      31/12/2018 06:32:14 AM                ASYCUDA Issue- 6. One User   \n",
       "1      31/12/2018 06:29:19 AM               Email Service Not Available   \n",
       "2      31/12/2018 06:27:18 AM                ORACLE ISSUE - 7. One User   \n",
       "3      31/12/2018 06:21:53 AM              PC related issues Â Airport   \n",
       "4      31/12/2018 06:19:02 AM                           BTP Not working   \n",
       "...                       ...                                       ...   \n",
       "22532           1/1/2018 8:48    Printer Issue Airport Â Laser Network   \n",
       "22533           1/1/2018 8:39           Printer Issue tÂ Laser Network   \n",
       "22534           1/1/2018 8:23  Duty Free System Issue -  Mobile Devices   \n",
       "22535           1/1/2018 8:19                           BTP Not working   \n",
       "22537                     NaN                                       NaN   \n",
       "\n",
       "                                                   Notes  \\\n",
       "0      1. Error Message \\n ASYKUDA is not functioning...   \n",
       "1      Error  - outlook is not opening \\n2018\\nULKBL0...   \n",
       "2      Error Message:\\n Oracle is out of order \\n2132...   \n",
       "3      dialog tv is out of order. \\ns/diva\\n4973\\nseedya   \n",
       "4         Counter and Airline - c 07. \\n4507\\nUL\\nAsanka   \n",
       "...                                                  ...   \n",
       "22532  Counter 15 printer issue\\nContact Number  4508...   \n",
       "22533  Multiple Printers are not working maintenance ...   \n",
       "22534  Device Type - mobile device.\\n\\nDevice Number(...   \n",
       "22535                                         Counter 28   \n",
       "22537                                                NaN   \n",
       "\n",
       "                                              Resolution  \\\n",
       "0              cleared the thinapp. and browder history.   \n",
       "1                   Âoutlook/e-mail  profile createdÂ.   \n",
       "2              refreshed session and cleared the browser   \n",
       "3                                       payment not done   \n",
       "4                             printer emulator restarted   \n",
       "...                                                  ...   \n",
       "22532  They had replaced torner after explaining that...   \n",
       "22533                           replaced the fuser unit.   \n",
       "22534  Advise user to correctly assign his position t...   \n",
       "22535                      BTP working fine. KB repalced   \n",
       "22537                                                NaN   \n",
       "\n",
       "               Assigned Group*+                            Assignee+  \\\n",
       "0          SD Incident Handling                   Sathischandra B.D.   \n",
       "1          SD Incident Handling                   Sathischandra B.D.   \n",
       "2          SD Incident Handling                   Sathischandra B.D.   \n",
       "3                           DCS                 Sudath Rohana ROHANA   \n",
       "4                           DCS                 Sudath Rohana ROHANA   \n",
       "...                         ...                                  ...   \n",
       "22532                       DCS             Upul Gunaratne GUNARATNE   \n",
       "22533       DMS Printer Support                  DMS Printer Support   \n",
       "22534  Duty Free System Support          Ishan Kularathna KULARATHNA   \n",
       "22535                       DCS  Dilanka Chandrasekara CHANDRASEKARA   \n",
       "22537                       NaN                                  NaN   \n",
       "\n",
       "                    Service*+ Submitter* Status*            First Name+  \\\n",
       "0                     ASYCUDA      11440  Closed                Donalad   \n",
       "1                       Email      11440  Closed                  Sarah   \n",
       "2                  Oracle EBS      11440  Closed               Praneeth   \n",
       "3             Desktop Service      11440  Closed                AIRPORT   \n",
       "4      File and Print Service      11440  Closed                AIRPORT   \n",
       "...                       ...        ...     ...                    ...   \n",
       "22532  File and Print Service     C13038  Closed     Sachinika Fernando   \n",
       "22533  File and Print Service     C13038  Closed  Mahinda Karunachandra   \n",
       "22534                     NaN     C12943  Closed      Primrose Arendtsz   \n",
       "22535  File and Print Service      21977  Closed                AIRPORT   \n",
       "22537                     NaN        NaN     NaN                    NaN   \n",
       "\n",
       "          Last Name+ Priority*       SLM Real Time Status  \\\n",
       "0          Jayaratne       Low                        NaN   \n",
       "1        Dissanayake       Low                        NaN   \n",
       "2           De Alwis       Low                        NaN   \n",
       "3               USER    Medium  Within the Service Target   \n",
       "4               USER    Medium  Within the Service Target   \n",
       "...              ...       ...                        ...   \n",
       "22532       FERNANDO    Medium  Within the Service Target   \n",
       "22533  KARUNACHANDRA    Medium   Service Targets Breached   \n",
       "22534       ARENDTSZ    Medium                        NaN   \n",
       "22535           USER    Medium  Within the Service Target   \n",
       "22537            NaN       NaN                        NaN   \n",
       "\n",
       "                    Organization  \\\n",
       "0                          CARGO   \n",
       "1                    ENGINEERING   \n",
       "2                    ENGINEERING   \n",
       "3      AIRPORT & GROUND SERVICES   \n",
       "4      AIRPORT & GROUND SERVICES   \n",
       "...                          ...   \n",
       "22532  AIRPORT & GROUND SERVICES   \n",
       "22533                ENGINEERING   \n",
       "22534             CABIN SERVICES   \n",
       "22535  AIRPORT & GROUND SERVICES   \n",
       "22537                        NaN   \n",
       "\n",
       "                                            Department  \n",
       "0                                          CARGO SALES  \n",
       "1                                 AIRCRAFT MAINTENANCE  \n",
       "2      ENGINEERING MATERIALS & SUPPLY CHAIN MANAGEMENT  \n",
       "3                             AIRPORT SERVICE DELIVERY  \n",
       "4                             AIRPORT SERVICE DELIVERY  \n",
       "...                                                ...  \n",
       "22532                         AIRPORT SERVICE DELIVERY  \n",
       "22533                             AIRCRAFT MAINTENANCE  \n",
       "22534                        INFLIGHT SERVICE DELIVERY  \n",
       "22535                         AIRPORT SERVICE DELIVERY  \n",
       "22537                                              NaN  \n",
       "\n",
       "[22536 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "df.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19392, 17)\n",
      "No of Rows: 19392\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   5\n",
      "Resolution                            164\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             393\n",
      "Service*+                            7957\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                 1096\n",
      "Organization                           14\n",
      "Department                             16\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2019_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14635, 17)\n",
      "No of Rows: 14635\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   5\n",
      "Resolution                             69\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             261\n",
      "Service*+                            5088\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                 1264\n",
      "Organization                           28\n",
      "Department                             32\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2020_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14819, 17)\n",
      "No of Rows: 14819\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   3\n",
      "Resolution                            101\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             400\n",
      "Service*+                            6351\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                 2088\n",
      "Organization                           16\n",
      "Department                             21\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2021_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14843, 17)\n",
      "No of Rows: 14843\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   4\n",
      "Resolution                             85\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             386\n",
      "Service*+                            5856\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                 1373\n",
      "Organization                           19\n",
      "Department                             23\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2022_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12543, 17)\n",
      "No of Rows: 12543\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1       1\n",
      "Incident ID*+                           2\n",
      "Submit Date                             2\n",
      "Summary*                                2\n",
      "Notes                                   5\n",
      "Resolution                            112\n",
      "Assigned Group*+                        2\n",
      "Assignee+                             414\n",
      "Service*+                            5832\n",
      "Submitter*                              2\n",
      "Status*                                 2\n",
      "First Name+                             2\n",
      "Last Name+                              2\n",
      "Priority*                               2\n",
      "SLM Real Time Status                  678\n",
      "Organization                            8\n",
      "Department                             32\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\2023_dataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new final dataset explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98770, 17)\n",
      "No of Rows: 98770\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        8\n",
      "Incident ID*+                           14\n",
      "Submit Date                             14\n",
      "Summary*                                14\n",
      "Notes                                   29\n",
      "Resolution                             718\n",
      "Assigned Group*+                        14\n",
      "Assignee+                             2067\n",
      "Service*+                            35653\n",
      "Submitter*                              14\n",
      "Status*                                 14\n",
      "First Name+                             14\n",
      "Last Name+                              14\n",
      "Priority*                               14\n",
      "SLM Real Time Status                  9789\n",
      "Organization                            99\n",
      "Department                             150\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\newdataset.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Operational Categorization Tier 1    Incident ID*+      Submit Date  \\\n",
      "count                              98762            98756            98756   \n",
      "unique                                 9            98756            98200   \n",
      "top                              Failure  INC000000416368  9/10/2018 15:57   \n",
      "freq                               97036                1                8   \n",
      "\n",
      "           Summary*                                Notes  \\\n",
      "count         98756                                98741   \n",
      "unique         1653                                96065   \n",
      "top     Cloud Issue  Issue : \\nStaff Number :\\nContact :   \n",
      "freq           8827                                   44   \n",
      "\n",
      "                                  Resolution     Assigned Group*+  \\\n",
      "count                                  98052                98756   \n",
      "unique                                 57628                  114   \n",
      "top     Âoutlook/e-mail  profile createdÂ.  KBSL Onsite Support   \n",
      "freq                                     979                26571   \n",
      "\n",
      "                 Assignee+        Service*+                  Submitter*  \\\n",
      "count                96703            63117                       98756   \n",
      "unique                 289              101                          79   \n",
      "top     IT Network Support  Desktop Service  Remedy Application Service   \n",
      "freq                  7039            26759                       31472   \n",
      "\n",
      "       Status* First Name+ Last Name+ Priority*       SLM Real Time Status  \\\n",
      "count    98756       98756      98756     98756                      88981   \n",
      "unique       3        4585       3936         4                          3   \n",
      "top     Closed     AIRPORT       USER    Medium  Within the Service Target   \n",
      "freq     96212        4209       4209     76183                      73470   \n",
      "\n",
      "                     Organization                Department  \n",
      "count                       98671                     98620  \n",
      "unique                         38                       132  \n",
      "top     AIRPORT & GROUND SERVICES  AIRPORT SERVICE DELIVERY  \n",
      "freq                        26437                     20608  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows before removal: 7\n",
      "Number of duplicate rows after removal: 0\n",
      "Duplicate rows have been removed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check the number of duplicate rows before removal\n",
    "initial_duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows before removal: {initial_duplicate_count}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Check the number of duplicate rows after removal\n",
    "final_duplicate_count = df_no_duplicates.duplicated().sum()\n",
    "print(f\"Number of duplicate rows after removal: {final_duplicate_count}\")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_no_duplicates.to_csv(\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\incident_report_new.csv\", index=False)  # Replace with desired output file path\n",
    "\n",
    "print(\"Duplicate rows have been removed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore after removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98763, 17)\n",
      "No of Rows: 98763\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        1\n",
      "Incident ID*+                            7\n",
      "Submit Date                              7\n",
      "Summary*                                 7\n",
      "Notes                                   22\n",
      "Resolution                             711\n",
      "Assigned Group*+                         7\n",
      "Assignee+                             2060\n",
      "Service*+                            35646\n",
      "Submitter*                               7\n",
      "Status*                                  7\n",
      "First Name+                              7\n",
      "Last Name+                               7\n",
      "Priority*                                7\n",
      "SLM Real Time Status                  9782\n",
      "Organization                            92\n",
      "Department                             143\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_new.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Missing Values\n",
    "* This remove subset of having missing values for resolution. So most of columns will have 0 missings after this. Others reduce there missing values count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing_resolution = df.dropna(subset=[\"Resolution\"])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_no_missing_resolution.to_csv(\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_new2.csv\", index=False)  # Replace with desired output file path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore After Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98052, 17)\n",
      "No of Rows: 98052\n",
      "\n",
      "Number of columns in the dataset: 17 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Notes                                   12\n",
      "Resolution                               6\n",
      "Assigned Group                           0\n",
      "Assignee                              1620\n",
      "Service                              35103\n",
      "Submitter                                0\n",
      "Status                                   0\n",
      "First Name                               0\n",
      "Last Name                                0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                  9694\n",
      "Organization                            84\n",
      "Department                             135\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final full dataset (new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After filtering necessary columns only for further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98052, 12)\n",
      "No of Rows: 98052\n",
      "\n",
      "Number of columns in the dataset: 12 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                               6\n",
      "Assigned Group                           0\n",
      "Service                              35103\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                  9694\n",
      "Organization                            84\n",
      "Department                             135\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Question and Answer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98052, 2)\n",
      "No of Rows: 98052\n",
      "\n",
      "Number of columns in the dataset: 2 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Summary       0\n",
      "Resolution    6\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "36790\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_q&a.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Summary and Resolution for Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_q&a.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "# Display initial information about the dataset\n",
    "initial_info = {\n",
    "    \"initial_shape\": df.shape,\n",
    "    \"columns\": df.columns.tolist(),\n",
    "    \"missing_values\": df.isnull().sum(),\n",
    "    \"duplicates_count\": df.duplicated().sum()\n",
    "}\n",
    "# 1. Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "# 2. Handle missing values by dropping rows with missing data\n",
    "df = df.dropna()\n",
    "# 3. Text cleaning: convert text to lowercase, remove punctuation, and strip whitespace\n",
    "df['Summary'] = df['Summary'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True).str.strip()\n",
    "df['Resolution'] = df['Resolution'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True).str.strip()\n",
    "# Display final shape and sample data after preprocessing\n",
    "final_info = {\n",
    "    \"final_shape\": df.shape,\n",
    "    \"sample_data\": df.head()\n",
    "}\n",
    "\n",
    "initial_info, final_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Dataset for Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "output_path = 'd:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\cleaned_incident_report_q&a.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"File saved as:\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore Final Full Question and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61256, 2)\n",
      "No of Rows: 61256\n",
      "\n",
      "Number of columns in the dataset: 2 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Summary       0\n",
      "Resolution    0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_q&a_cleaned.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Rows where 'Organization' or 'Department' Columns have Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "# Drop rows where 'Organization' or 'Department' columns have missing values\n",
    "cleaned_data = df.dropna(subset=['Organization', 'Department'])\n",
    "\n",
    "# Save the cleaned dataset to a new file\n",
    "cleaned_file_path = 'd:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_model1.csv'\n",
    "cleaned_data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "cleaned_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore After Remove Missing Values in Dpartment and Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97917, 12)\n",
      "No of Rows: 97917\n",
      "\n",
      "Number of columns in the dataset: 12 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                               6\n",
      "Assigned Group                           0\n",
      "Service                              35073\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                  9689\n",
      "Organization                             0\n",
      "Department                               0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_model1.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker to generate realistic synthetic data\n",
    "fake = Faker()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_model1.csv\"\n",
    "\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "# Define the number of synthetic rows needed\n",
    "additional_rows_needed = 98000 - df.shape[0]\n",
    "\n",
    "# Function to create a synthetic row with similar characteristics to the original data\n",
    "def create_synthetic_row():\n",
    "    return {\n",
    "        'Operational Categorization Tier 1': np.random.choice(df['Operational Categorization Tier 1']),\n",
    "        'Incident ID': f\"INC{fake.random_int(min=100000000, max=999999999)}\",\n",
    "        'Submit Date': fake.date_time_this_decade().strftime('%d/%m/%Y %I:%M:%S %p'),\n",
    "        'Summary': fake.sentence(nb_words=5),\n",
    "        'Resolution': fake.sentence(nb_words=5) if np.random.rand() > 0.1 else np.nan,  # Some rows with missing resolutions\n",
    "        'Assigned Group': np.random.choice(df['Assigned Group'].dropna()),\n",
    "        'Service': np.random.choice(df['Service'].dropna()) if np.random.rand() > 0.3 else np.nan,  # Some missing 'Service' values\n",
    "        'Status': np.random.choice(df['Status'].dropna()),\n",
    "        'Priority': np.random.choice(df['Priority'].dropna()),\n",
    "        'SLM Real Time Status': np.random.choice(df['SLM Real Time Status'].dropna()) if np.random.rand() > 0.1 else np.nan,\n",
    "        'Organization': np.random.choice(df['Organization'].dropna()),\n",
    "        'Department': np.random.choice(df['Department'].dropna())\n",
    "    }\n",
    "\n",
    "# Generate the synthetic rows\n",
    "synthetic_rows = [create_synthetic_row() for _ in range(additional_rows_needed)]\n",
    "synthetic_data = pd.DataFrame(synthetic_rows)\n",
    "\n",
    "# Concatenate the original data with the synthetic data\n",
    "expanded_data = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "\n",
    "# Verify there are no duplicate rows\n",
    "expanded_data = expanded_data.drop_duplicates()\n",
    "\n",
    "# Save the expanded dataset to a new file\n",
    "expanded_file_path = 'd:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000.csv'\n",
    "expanded_data.to_csv(expanded_file_path, index=False)\n",
    "\n",
    "expanded_data.shape, expanded_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 12)\n",
      "No of Rows: 98000\n",
      "\n",
      "Number of columns in the dataset: 12 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                              21\n",
      "Assigned Group                           0\n",
      "Service                              35100\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                  9695\n",
      "Organization                             0\n",
      "Department                               0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Realistic Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 12)\n",
      "No of Rows: 98000 \n",
      "\n",
      "Number of columns in the dataset: 12 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                               6\n",
      "Assigned Group                           0\n",
      "Service                              35098\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                  9693\n",
      "Organization                             0\n",
      "Department                               0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker to generate realistic synthetic data\n",
    "fake = Faker()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = \"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_model1.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1')\n",
    "\n",
    "# Calculate the number of additional rows needed to reach 98,000\n",
    "additional_rows_needed = 98000 - df.shape[0]\n",
    "\n",
    "# Function to create a synthetic row with similar characteristics to the original data\n",
    "def create_synthetic_row():\n",
    "    return {\n",
    "        'Operational Categorization Tier 1': np.random.choice(df['Operational Categorization Tier 1']),\n",
    "        'Incident ID': f\"INC{fake.random_int(min=100000000, max=999999999)}\",\n",
    "        'Submit Date': fake.date_time_this_decade().strftime('%d/%m/%Y %I:%M:%S %p'),\n",
    "        'Summary': fake.sentence(nb_words=5),\n",
    "        'Resolution': fake.sentence(nb_words=5),  # Ensure every row has a Resolution value\n",
    "        'Assigned Group': np.random.choice(df['Assigned Group'].dropna()),\n",
    "        'Service': np.random.choice(df['Service'].dropna()) if np.random.rand() > 0.3 else np.nan,  # Some missing 'Service' values\n",
    "        'Status': np.random.choice(df['Status'].dropna()),\n",
    "        'Priority': np.random.choice(df['Priority'].dropna()),\n",
    "        'SLM Real Time Status': np.random.choice(df['SLM Real Time Status'].dropna()) if np.random.rand() > 0.1 else np.nan,\n",
    "        'Organization': np.random.choice(df['Organization'].dropna()),\n",
    "        'Department': np.random.choice(df['Department'].dropna())\n",
    "    }\n",
    "\n",
    "# Generate the synthetic rows\n",
    "synthetic_rows = [create_synthetic_row() for _ in range(additional_rows_needed)]\n",
    "synthetic_data = pd.DataFrame(synthetic_rows)\n",
    "\n",
    "# Concatenate the original data with the synthetic data\n",
    "expanded_data = pd.concat([df, synthetic_data], ignore_index=True)\n",
    "\n",
    "# Verify there are no duplicate rows\n",
    "expanded_data = expanded_data.drop_duplicates()\n",
    "\n",
    "# Save the expanded dataset to a new file\n",
    "expanded_file_path = \"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000.csv\"\n",
    "expanded_data.to_csv(expanded_file_path, index=False)\n",
    "\n",
    "# Print summary of the dataset\n",
    "print(expanded_data.shape)  # Verify the new shape of the data\n",
    "print(\"No of Rows:\", expanded_data.shape[0], \"\\n\")\n",
    "print(\"Number of columns in the dataset:\", expanded_data.shape[1], \"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(expanded_data.isnull().sum())   \n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(expanded_data.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill Missing Values in the 'SLM Real Time Status' Column and 'Resolution' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15004\\1969260417.py:17: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['SLM Real Time Status'].fillna(method='ffill', inplace=True)  # forward fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 12)\n",
      "\n",
      "Missing values per column after filling:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                               0\n",
      "Assigned Group                           0\n",
      "Service                              35098\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                     3\n",
      "Organization                             0\n",
      "Department                               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the expanded dataset\n",
    "dataset_path = \"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000.csv\"\n",
    "df = pd.read_csv(dataset_path, encoding='latin1')\n",
    "\n",
    "# Fill missing values in the 'Resolution' column'Resolution' column with the most frequent value\n",
    "resolution_mode = df['Resolution'].mode()[0]\n",
    "df['Resolution'].fillna(resolution_mode, inplace=True)\n",
    "\n",
    "# Fill missing values in the 'SLM Real Time Status' column\n",
    "df['SLM Real Time Status'].fillna(method='ffill', inplace=True)  # forward fill\n",
    "# df['SLM Real Time Status'].fillna(\"Not Available\", inplace=True)  # placeholder\n",
    "\n",
    "# Save the updated dataset\n",
    "cleaned_file_path = \"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Print summary of the dataset to verify\n",
    "print(df.shape)  # Verify the shape\n",
    "print(\"\\nMissing values per column after filling:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explore Final Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98000, 13)\n",
      "No of Rows: 98000\n",
      "\n",
      "Number of columns in the dataset: 13 \n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Operational Categorization Tier 1        0\n",
      "Incident ID                              0\n",
      "Submit Date                              0\n",
      "Summary                                  0\n",
      "Resolution                               0\n",
      "Assigned Group                           0\n",
      "Service                              35098\n",
      "Submitter                                0\n",
      "Status                                   0\n",
      "Priority                                 0\n",
      "SLM Real Time Status                     0\n",
      "Organization                             0\n",
      "Department                               0\n",
      "dtype: int64\n",
      "\n",
      "Number of duplicate rows:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset=\"d:\\\\User Data\\\\Oshadi\\\\USJ\\\\Acedemic\\\\3rd Year\\\\Sem 6\\\\Project\\\\Test1\\\\clean_and_encoding\\\\dataset\\\\New dataset\\\\incident_report_preprocessed_final_98000_cleaned.csv\"\n",
    "df = pd.read_csv(dataset, encoding='latin1') \n",
    "\n",
    "\n",
    "print(df.shape) #see number of rows and columns\n",
    "num_rows = df.shape[0]\n",
    "print(\"No of Rows: \" + str(df.shape[0])+\"\\n\")\n",
    "num_columns = df.shape[1]\n",
    "print(\"Number of columns in the dataset:\", num_columns,\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())   \n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\nNumber of duplicate rows:\")   \n",
    "print(df.duplicated().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
